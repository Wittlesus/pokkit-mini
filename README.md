# pokkit-mini

Fine-tuned small language model for the Pokkit AI agent. Trained specifically on phone-control tasks, tool calling, screen automation, and Pokkit's personality.

## Base Model

**Qwen3-4B-Instruct-2507** — strong tool calling and reasoning at 4B params.
Fallback: **Qwen3-8B** (8B) — more capacity if 4B underperforms.

## Why fine-tune?

A generic model wastes tokens figuring out what Pokkit can do. pokkit-mini:
- Already knows all built-in tools and their exact schemas
- Calls tools immediately without hedging
- Has Pokkit's personality baked in (no system prompt needed for basics)
- Understands screen control workflows (read → tap → verify)
- Refuses background automation correctly
- Understands custom plugin patterns

## Training

- **Method**: LoRA (rank 32, alpha 64) via Unsloth — 4x faster, runs on free Colab T4/A100
- **Dataset**: ~3,600+ synthetic conversations generated by `generate_dataset.py`
- **Epochs**: 3 (with early stopping)
- **Learning rate**: 5e-5
- **Max sequence length**: 4096
- **Time**: ~45 min on T4, ~15 min on A100

## Files

```
pokkit-mini/
  generate_dataset.py     <- Synthetic training data generator
  dataset_core.py         <- Shared helpers, tool schemas, system prompt
  dataset_batch*.py       <- Topic-specific training data generators
  clean_dataset.py        <- Dedup, validate, and rebalance
  train.py                <- Unsloth LoRA fine-tuning script
  export.py               <- Export to GGUF (Ollama) + ONNX
  eval_model.py           <- Evaluation suite (37 tests, 10 categories)
  Modelfile               <- Ollama Modelfile for distribution
  data/
    train.jsonl           <- Generated training data
    eval.jsonl            <- Held-out evaluation set
  notebooks/
    train_colab.ipynb     <- Google Colab notebook (free T4/A100 GPU)
```

## Quick Start

### Generate training data
```bash
python generate_dataset.py --output data/train.jsonl --count 5000
python clean_dataset.py --input data/train.jsonl --output data/train_clean.jsonl
```

### Train (local GPU)
```bash
pip install unsloth
python train.py --model qwen3-4b --data data/train_clean.jsonl --output ./pokkit-mini-lora
```

### Train (Google Colab)
Open `notebooks/train_colab.ipynb` in Colab, select T4/A100 GPU runtime, run all cells.

### Export to Ollama
```bash
python export.py --lora ./pokkit-mini-lora --format gguf --quant q5_k_m --output ./pokkit-mini.gguf
ollama create pokkit-mini -f Modelfile
ollama run pokkit-mini
```

### Use in Pokkit app
In Settings -> Provider -> Ollama -> Model -> `pokkit-mini`

## Dataset Format

Each example is a conversation in ChatML format with OpenAI-compatible tool calls:

```json
{
  "messages": [
    {"role": "system", "content": "You are Pokkit..."},
    {"role": "user", "content": "Set an alarm for 7am tomorrow"},
    {"role": "assistant", "content": null, "tool_calls": [
      {"id": "call_abc123", "type": "function", "function": {"name": "set_alarm", "arguments": "{\"hour\": 7, \"minute\": 0, \"label\": \"Wake up\"}"}}
    ]},
    {"role": "tool", "tool_call_id": "call_abc123", "name": "set_alarm", "content": "{\"success\": true}"},
    {"role": "assistant", "content": "7am alarm locked in! \ud83d\udc38 you've got this tomorrow."}
  ],
  "tools": [...]
}
```

## Tools

Production tools (from `pokkit/go-core/tools/`):
- `set_alarm` — hour/minute/label
- `show_notification` — title/body
- `write_clipboard` / `read_clipboard`
- `screen_read` / `screen_tap` / `screen_type` / `screen_scroll` / `screen_back` / `screen_home` / `screen_find_and_tap`

Stub tools (to be implemented):
- `web_search` — query
- `take_note` — title/content
- `store_value` / `retrieve_value` — key/value memory
