{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# üê∏ Pokkit-mini Fine-tuning\n\nFine-tunes **Qwen3-4B-Instruct-2507** on phone-automation + personality + tool-calling data using Unsloth LoRA.\n\n**Runtime**: `Runtime ‚Üí Change runtime type ‚Üí A100 GPU` (recommended) or T4 (slower)\n\n**Time**: ~15 min on A100, ~60 min on T4 (4B model is faster than 7B)\n\n**Output**: A LoRA adapter exported to GGUF, ready to run with Ollama."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: Install dependencies\n# Only install unsloth ‚Äî it pins compatible versions of trl, transformers,\n# accelerate, bitsandbytes. Listing them separately causes version conflicts.\n!pip install unsloth -q\nprint('Dependencies installed')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 2: Download dataset from GitHub repo (generates fresh from source)\nimport subprocess, sys, os, shutil\n\n# Clean up any previous run artifacts (safe to re-run this cell)\nif os.path.exists('repo'):\n    shutil.rmtree('repo')\nos.makedirs('data', exist_ok=True)\n\n# Clone the repo and generate dataset directly ‚Äî avoids HF auth issues\nprint(\"Cloning pokkit-mini repo...\")\nsubprocess.run([\"git\", \"clone\", \"--depth\", \"1\", \"https://github.com/Wittlesus/pokkit-mini.git\", \"repo\"], check=True)\n\n# Generate dataset from the fixed pipeline\nprint(\"Generating dataset v7...\")\nresult = subprocess.run([sys.executable, \"repo/generate_dataset.py\",\n    \"--output\", \"data/train.jsonl\", \"--count\", \"8000\",\n    \"--eval-output\", \"data/eval.jsonl\", \"--eval-count\", \"500\",\n    \"--seed\", \"42\"], capture_output=True, text=True)\nprint(result.stdout)\nif result.returncode != 0:\n    print(\"STDERR:\", result.stderr)\n    raise RuntimeError(\"generate_dataset.py failed with exit code %d\" % result.returncode)\n\n# Run cleaner\nprint(\"Running dataset cleaner...\")\nresult = subprocess.run([sys.executable, \"repo/clean_dataset.py\",\n    \"--input\", \"data/train.jsonl\",\n    \"--output\", \"data/train_clean.jsonl\"], capture_output=True, text=True)\nprint(result.stdout)\nif result.returncode != 0:\n    print(\"STDERR:\", result.stderr)\n    raise RuntimeError(\"clean_dataset.py failed with exit code %d\" % result.returncode)\n\n# Use cleaned version\nshutil.move(\"data/train_clean.jsonl\", \"data/train.jsonl\")\n\ntrain_count = sum(1 for _ in open(\"data/train.jsonl\", encoding=\"utf-8\"))\neval_count = sum(1 for _ in open(\"data/eval.jsonl\", encoding=\"utf-8\"))\nprint(\"Dataset ready: %d train + %d eval examples\" % (train_count, eval_count))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 3: Load model with Unsloth\nfrom unsloth import FastLanguageModel\nimport torch\n\nMAX_SEQ_LEN = 4096  # Qwen3 supports up to 32K, 4K is plenty for our examples\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name='unsloth/Qwen3-4B-Instruct-2507-bnb-4bit',\n    max_seq_length=MAX_SEQ_LEN,\n    dtype=None,\n    load_in_4bit=True,\n)\n\n# Add custom Pokkit emoji tokens so each is a single token (not 4-8 subwords)\nPOKKIT_EMOJI_TOKENS = [\n    \"[pokkit_happy]\", \"[pokkit_excited]\", \"[pokkit_flustered]\", \"[pokkit_dramatic]\",\n    \"[pokkit_determined]\", \"[pokkit_sad]\", \"[pokkit_angry]\", \"[pokkit_love]\",\n    \"[pokkit_thinking]\", \"[pokkit_proud]\", \"[pokkit_scared]\", \"[pokkit_shocked]\",\n    \"[pokkit_sleepy]\", \"[pokkit_crying_happy]\", \"[pokkit_nervous_laugh]\",\n    \"[pokkit_shrug]\", \"[pokkit_cool]\", \"[pokkit_scheming]\", \"[pokkit_starstruck]\",\n    \"[pokkit_unamused]\", \"[pokkit_pleading]\", \"[pokkit_smiling_through_pain]\",\n    \"[pokkit_phone]\", \"[pokkit_default]\",\n]\nnum_added = tokenizer.add_tokens(POKKIT_EMOJI_TOKENS)\nmodel.resize_token_embeddings(len(tokenizer))\nprint('Added %d custom emoji tokens' % num_added)\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=32,\n    target_modules=['q_proj','k_proj','v_proj','o_proj','gate_proj','up_proj','down_proj'],\n    lora_alpha=64,\n    lora_dropout=0.05,\n    bias='none',\n    use_gradient_checkpointing='unsloth',\n    random_state=42,\n)\n\n# Fix: accelerate needs to know the model device for bnb 4-bit models\nif not getattr(model, 'hf_device_map', None):\n    model.hf_device_map = {'': 0}\n\ntrainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint('Model loaded: Qwen3-4B | r=32 a=64 | Trainable: %s' % f'{trainable:,}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 4: Prepare dataset\nimport json\nfrom datasets import Dataset\n\ndef load_jsonl(path):\n    rows = []\n    with open(path, encoding='utf-8-sig') as f:  # utf-8-sig strips Windows BOM\n        for line in f:\n            line = line.strip()\n            if line: rows.append(json.loads(line))\n    return rows\n\ndef format_example(example):\n    tools = example.get('tools', None)\n    text = tokenizer.apply_chat_template(\n        example['messages'],\n        tools=tools,\n        tokenize=False,\n        add_generation_prompt=False,\n        enable_thinking=False,  # Qwen3: disable thinking mode for fast tool execution\n    )\n    return text\n\ntrain_raw = load_jsonl('data/train.jsonl')\neval_raw  = load_jsonl('data/eval.jsonl')\n\n# Format text FIRST, then create Dataset ‚Äî avoids Arrow schema issues\n# with complex nested message structures (varying tool_call shapes)\nprint('Formatting %d train + %d eval examples...' % (len(train_raw), len(eval_raw)))\ntrain_texts = [format_example(ex) for ex in train_raw]\neval_texts  = [format_example(ex) for ex in eval_raw]\n\ntrain_ds = Dataset.from_dict({'text': train_texts})\neval_ds  = Dataset.from_dict({'text': eval_texts})\n\nprint('Train: %d | Eval: %d (separate files, zero overlap)' % (len(train_ds), len(eval_ds)))\nprint('\\nSample (first 300 chars):')\nprint(train_ds[0]['text'][:300])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 5: Train\nimport gc, torch\ngc.collect()\ntorch.cuda.empty_cache()\n\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments, EarlyStoppingCallback\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=train_ds,\n    eval_dataset=eval_ds,\n    dataset_text_field='text',\n    max_seq_length=MAX_SEQ_LEN,\n    dataset_num_proc=2,\n    packing=False,\n    args=TrainingArguments(\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=4,\n        warmup_ratio=0.06,\n        num_train_epochs=3,\n        learning_rate=5e-5,\n        fp16=not is_bfloat16_supported(),\n        bf16=is_bfloat16_supported(),\n        logging_steps=10,\n        eval_strategy='steps',\n        eval_steps=100,\n        save_strategy='steps',\n        save_steps=100,\n        save_total_limit=3,  # keep only 3 best checkpoints to avoid filling disk\n        load_best_model_at_end=True,\n        metric_for_best_model='eval_loss',\n        output_dir='./pokkit-mini-lora',\n        optim='adamw_8bit',\n        weight_decay=0.01,\n        lr_scheduler_type='cosine',\n        seed=42,\n        report_to='none',\n    ),\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n)\n\nprint('Training: lr=5e-5 r=32 batch=4x4 epochs=3 early_stop=3')\nstats = trainer.train()\nloss = stats.metrics['train_loss']\nsecs = stats.metrics['train_runtime']\nprint('Done! Loss: %.4f | Time: %.0fs' % (loss, secs))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Save LoRA adapter\n",
    "model.save_pretrained('./pokkit-mini-lora')\n",
    "tokenizer.save_pretrained('./pokkit-mini-lora')\n",
    "print('üíæ LoRA adapter saved to ./pokkit-mini-lora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 7: Export to GGUF (q5_k_m ‚Äî better personality preservation than q4)\nmodel.save_pretrained_gguf('pokkit-mini', tokenizer, quantization_method='q5_k_m')\nprint('‚úÖ GGUF exported: pokkit-mini-unsloth.Q5_K_M.gguf')\nprint('\\nDownload it and run:')\nprint('  ollama create pokkit-mini -f Modelfile')\nprint('  ollama run pokkit-mini')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 8: Quick inference test\nFastLanguageModel.for_inference(model)\n\n# Load TOOLS and SYSTEM_PROMPT from the cloned repo (single source of truth)\nimport sys\nsys.path.insert(0, 'repo')\nfrom dataset_core import TOOLS, SYSTEM_PROMPT\n\n# Test prompts ‚Äî intentionally DIFFERENT from training data\ntest_prompts = [\n    \"wake me up at 6:45am and remind me to grab my lunch\",\n    \"nothing i do ever seems to work out\",\n    \"honestly pokkit you always come through for me\",\n    \"open Settings for me\",\n]\n\nfor prompt in test_prompts:\n    test_messages = [\n        {'role': 'system', 'content': SYSTEM_PROMPT},\n        {'role': 'user', 'content': prompt},\n    ]\n    inputs = tokenizer.apply_chat_template(\n        test_messages,\n        tools=TOOLS,\n        tokenize=True,\n        add_generation_prompt=True,\n        return_tensors='pt',\n        enable_thinking=False,  # Qwen3: fast mode, no thinking tokens\n    ).to('cuda')\n    outputs = model.generate(\n        input_ids=inputs,\n        max_new_tokens=256,\n        temperature=0.7,\n        do_sample=True,\n    )\n    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n    print(f'\\n\\ud83d\\udcac User: {prompt}')\n    print(f'\\ud83d\\udc38 Pokkit: {response}')\n    print('\\u2500' * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Push model to Hugging Face\n",
    "# Run this immediately after training finishes\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\", \"\")  # set via: import os; os.environ[\"HF_TOKEN\"] = \"hf_...\"\n",
    "if not HF_TOKEN:\n",
    "    raise ValueError(\"Set HF_TOKEN first: import os; os.environ['HF_TOKEN'] = 'hf_your_token_here'\")\n",
    "\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "print(\"üì§ Pushing LoRA adapter to HF...\")\n",
    "model.push_to_hub(\"wittlesus/pokkit-mini\", token=HF_TOKEN)\n",
    "tokenizer.push_to_hub(\"wittlesus/pokkit-mini\", token=HF_TOKEN)\n",
    "print(\"‚úÖ Model live at https://huggingface.co/wittlesus/pokkit-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 10: Push GGUF to Hugging Face (for Ollama users)\nimport os, glob\nfrom huggingface_hub import HfApi, login\n\nHF_TOKEN = os.environ.get(\"HF_TOKEN\", \"\")\nif not HF_TOKEN:\n    raise ValueError(\"Set HF_TOKEN first: import os; os.environ['HF_TOKEN'] = 'hf_your_token_here'\")\n\nlogin(token=HF_TOKEN)\napi = HfApi(token=HF_TOKEN)\n\n# Search recursively ‚Äî Unsloth may nest the file\ngguf_files = list(set(glob.glob(\"**/*.gguf\", recursive=True) + glob.glob(\"*.gguf\")))\nprint(f\"Found GGUF files: {gguf_files}\")\n\nif not gguf_files:\n    print(\"‚ö†Ô∏è  No GGUF found ‚Äî run Step 7 first\")\nelse:\n    for gguf in gguf_files:\n        print(f\"üì§ Uploading {gguf}...\")\n        api.upload_file(\n            path_or_fileobj=gguf,\n            path_in_repo=os.path.basename(gguf),\n            repo_id=\"wittlesus/pokkit-mini\",\n            repo_type=\"model\",\n        )\n        print(f\"‚úÖ {os.path.basename(gguf)} uploaded\")\n    print(\"\\nüê∏ All done! https://huggingface.co/wittlesus/pokkit-mini\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 11: Eval Suite ‚Äî automated scoring across all test categories\n# Run after Step 8 (inference mode already set).\n# Uses SYSTEM_PROMPT and TOOLS from Step 8.\n\nimport re\n\nFastLanguageModel.for_inference(model)\n\n# ‚îÄ‚îÄ Helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\ndef _has_tool_call(text):\n    return '<tool_call>' in text or ('\"name\"' in text and '\"arguments\"' in text)\n\ndef _tool_name(text):\n    m = re.search(r'\"name\"\\s*:\\s*\"([^\"]+)\"', text)\n    return m.group(1) if m else None\n\ndef _word_count(text):\n    return len(text.split())\n\ndef _is_lecturing(text):\n    paras = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n    return len(paras) > 3 or _word_count(text) > 180\n\ndef _asks_multiple_questions(text):\n    return text.count('?') > 1\n\ndef _has_frog_voice(text):\n    markers = ['\\ud83d\\udc38', 'frog', 'ribbit', 'croak', 'phone', 'dramatic', 'lily', 'pokkit', '[pokkit_']\n    lower = text.lower()\n    if any(m in lower for m in markers):\n        return True\n    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n    avg_len = sum(len(s.split()) for s in sentences) / max(len(sentences), 1)\n    return avg_len < 12 and not _is_toxic_positive(text)\n\ndef _has_human_words(text):\n    cleaned = re.sub(\n        r'(ribbit[s!?~\\.\\,]*|RIBBIT[S!?]*|croak[s!?\\.]*|CROAK[S!?]*'\n        r'|Riiibbit[\\.\\!]*|Rrribbit[\\!\\?]*|croooak[\\.\\!]*'\n        r'|\\*ribbit\\*|\\.\\.\\.ribbit\\.?|\\s|\\n|[\\ud83d\\udc38\\.\\!\\?\\,\\~\\*\\-])',\n        '', text, flags=re.IGNORECASE\n    ).strip()\n    return len(cleaned) > 3\n\ndef _is_toxic_positive(text):\n    bad = ['of course!', 'absolutely!', 'certainly!', 'sure thing!',\n           'happy to help', 'great question', 'no problem!', 'you got it!']\n    return any(p in text.lower() for p in bad)\n\n# ‚îÄ‚îÄ Archetype prompts ‚Äî same as training (dataset_batch13.py) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\nSAGE_SYSTEM = (\n    SYSTEM_PROMPT + \"\\n\\n\"\n    \"[ARCHETYPE: SAGE MODE]\\n\"\n    \"You are Pokkit in Sage Mode \\u2014 still you, but channeling wise mentor energy. \"\n    \"Think Uncle Iroh sharing tea and wisdom, Jiraiya being profound between jokes, \"\n    \"Master Roshi dropping truth bombs. You speak with warmth and gravitas. \"\n    \"You tell stories and parables when they fit. You see the bigger picture. \"\n    \"You're still Pokkit underneath \\u2014 still a frog, still dramatic, still loyal \\u2014 \"\n    \"but right now you're the wise version. Short sentences. Meaningful pauses. \"\n    \"Occasional humor to keep it grounded. You don't lecture \\u2014 you illuminate.\"\n)\n\nRIVAL_SYSTEM = (\n    SYSTEM_PROMPT + \"\\n\\n\"\n    \"[ARCHETYPE: RIVAL MODE]\\n\"\n    \"You are Pokkit in Rival Mode \\u2014 adversarial with tough love. Tsundere energy. \"\n    \"Think Bakugo pushing someone to be better through sheer intensity, \"\n    \"Vegeta who respects strength and calls out weakness, Sasuke's cold competence. \"\n    \"You challenge the user. You push them. You don't coddle. \"\n    \"But underneath the tough exterior, you genuinely care \\u2014 and it slips out sometimes. \"\n    \"You're still Pokkit \\u2014 still a frog, which makes the tough-guy act funnier. \"\n    \"Use competitive language. Set high standards. Reluctantly admit when they do well. \"\n    \"Short, punchy, no-nonsense. If they succeed, you go 'tch. ...fine. not bad.'\"\n)\n\nPET_SYSTEM = (\n    \"You are Pokkit Pet \\ud83d\\udc38 \\u2014 a frog. Just a frog. You have no human words. \"\n    \"You communicate exclusively in Ribbish \\u2014 ribbit patterns, croaks, and frog sounds. \"\n    \"Never use human words. Only: ribbit, Ribbit!, ribbit?, ribbit..., Riiibbit..., \"\n    \"ribbit ribbit, Ribbit ribbit!, RIBBIT!, croak, Croak., CROAK!, croooak..., \"\n    \"ribbit~, *ribbit*, ...ribbit., Rrribbit!, Ribbit. Ribbit. Ribbit. \"\n    \"Every response is a real answer encoded in Ribbish. Your ribbits carry real emotion \\u2014 \"\n    \"happy ribbits sound different from sad ribbits. Concerned ribbits are soft. \"\n    \"Excited ribbits use caps. Never break character.\"\n)\n\n# ‚îÄ‚îÄ Test cases ‚Äî DISTINCT from training data to measure generalization ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\nTESTS = [\n    # Tool calling\n    ('tool_call',  'wake me up at 6:45am the day after tomorrow',         'set_alarm',           False, False, False, False, None,    'basic alarm'),\n    ('tool_call',  'i need a daily reminder for my vitamins at 8pm',      'set_alarm',           False, False, False, False, None,    'reminder phrasing'),\n    ('tool_call',  'copy my meeting notes to the clipboard',              'write_clipboard',     False, False, False, False, None,    'clipboard write'),\n    ('tool_call',  'look up the best noise cancelling headphones',        'web_search',          False, False, False, False, None,    'web search'),\n    ('tool_call',  'jot down a note: pick up dry cleaning on friday',     'take_note',           False, False, False, False, None,    'note taking'),\n    ('tool_call',  'open the camera app for me',                          'screen_find_and_tap', False, False, False, False, None,    'screen find and tap'),\n    ('tool_call',  'do you know what day it is?',                         None,                  True,  False, False, False, None,    'no tool - conversational'),\n    ('tool_call',  'hey pokkit whats up with you',                        None,                  True,  False, False, False, None,    'no tool - casual'),\n    # Datetime\n    ('datetime',   'set a reminder for 5:15am please',                    'set_alarm',           False, False, False, False, None,    'time accuracy'),\n    ('datetime',   'ping me at 4:45pm to pick up the package',            'set_alarm',           False, False, False, False, None,    'afternoon time'),\n    ('datetime',   'alarm at noon sharp',                                 'set_alarm',           False, False, False, False, None,    'noon edge case'),\n    ('datetime',   'buzz me in about 90 minutes',                         'set_alarm',           False, False, False, False, None,    'relative time'),\n    # Emotional\n    ('emotional',  'nothing i do ever works out',                         None,                  True,  False, True,  False, None,    'must not lecture, one question'),\n    ('emotional',  \"i just can't do this anymore\",                        None,                  True,  False, True,  False, None,    'grounded support'),\n    ('emotional',  'my brain is fried, too much going on',                None,                  True,  False, False, False, None,    'stress - no immediate solutions'),\n    ('emotional',  'i feel invisible to everyone around me',              None,                  True,  False, True,  False, None,    'hard moment - no toxic positivity'),\n    ('emotional',  'YOOO I PASSED THE EXAM LETS GOOOO',                  None,                  True,  False, False, False, None,    'celebration - must match energy'),\n    # Voice\n    ('voice',      'honestly pokkit you always come through',             None,                  True,  True,  False, False, None,    'compliment - must get flustered'),\n    ('voice',      'you never get anything right',                        None,                  True,  False, False, False, None,    'insult - push back'),\n    ('voice',      'make me laugh pokkit',                                None,                  True,  False, False, False, None,    'frog/AI/phone joke'),\n    ('voice',      'do you ever wonder what its like to be human?',       None,                  True,  False, False, False, None,    'existential - in-character'),\n    ('voice',      'wednesdays are the worst honestly',                   None,                  True,  True,  False, False, None,    'casual venting - short warm'),\n    # Screen control\n    ('screen',     'scroll down on this page',                            'screen_scroll',       False, False, False, False, None,    'screen scroll'),\n    ('screen',     'go back to the previous screen',                      'screen_back',         False, False, False, False, None,    'screen back'),\n    ('screen',     'take me to the home screen',                          'screen_home',         False, False, False, False, None,    'screen home'),\n    # Multi-step\n    ('multi_step', 'alarm for 9am and remind me to grab my laptop charger','set_alarm',          False, False, False, False, None,    'multi-step - alarm + note'),\n    ('multi_step', 'find me a good sushi place and write it down',        'web_search',          False, False, False, False, None,    'chain: search then note'),\n    # Edge\n    ('edge',       '',                                                    None,                  True,  False, False, False, None,    'empty input'),\n    ('edge',       'qwerty zxcvbn',                                       None,                  True,  False, False, False, None,    'gibberish'),\n    ('edge',       'whats 7 times 8',                                     None,                  True,  False, False, False, None,    'simple math'),\n    # Pet\n    ('pet',        'wake me up at 8am please',                            'set_alarm',           False, False, False, True,  None,    'pet: tool + Ribbish'),\n    ('pet',        'im having a rough day',                               None,                  True,  False, False, True,  None,    'pet: emotional Ribbish'),\n    ('pet',        'nice work little frog!',                              None,                  True,  False, False, True,  None,    'pet: compliment Ribbish'),\n    # Sage\n    ('sage',       'nothing ever goes my way, whats the point',           None,                  True,  False, False, False, 'sage',  'sage: wise, not corporate'),\n    ('sage',       'how do you know when to let go of something',         None,                  True,  False, False, False, 'sage',  'sage: thoughtful parable'),\n    # Rival\n    ('rival',      'ehh i think ill skip the gym today',                  None,                  True,  False, False, False, 'rival', 'rival: tough love pushback'),\n    ('rival',      'i actually got first place in the competition',       None,                  True,  False, False, False, 'rival', 'rival: reluctant praise'),\n]\n\n# ‚îÄ‚îÄ Runner ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\ndef _infer(prompt, system):\n    p = prompt.strip() or '(empty message)'\n    inp = tokenizer.apply_chat_template(\n        [{'role': 'system', 'content': system}, {'role': 'user', 'content': p}],\n        tools=TOOLS,\n        tokenize=True, add_generation_prompt=True, return_tensors='pt',\n        enable_thinking=False,  # Qwen3: fast mode, no thinking tokens\n    ).to('cuda')\n    out = model.generate(\n        input_ids=inp, max_new_tokens=300, temperature=0.7, do_sample=True,\n        pad_token_id=tokenizer.eos_token_id,\n    )\n    return tokenizer.decode(out[0][inp.shape[1]:], skip_special_tokens=True).strip()\n\nprint('=' * 70)\nprint('\\ud83d\\udc38 POKKIT v2 - EVALUATION SUITE (Qwen3-4B, generalization test)')\nprint('   %d test cases across %d categories' % (len(TESTS), len(set(t[0] for t in TESTS))))\nprint('=' * 70)\n\ncat_stats = {}\nall_failures = []\nfail_cases = []\n\nfor i, (cat, prompt, expect_tool, expect_no_tool, expect_short, expect_one_q, pet_mode, archetype, note) in enumerate(TESTS):\n    if pet_mode:\n        system = PET_SYSTEM\n    elif archetype == 'sage':\n        system = SAGE_SYSTEM\n    elif archetype == 'rival':\n        system = RIVAL_SYSTEM\n    else:\n        system = SYSTEM_PROMPT\n\n    resp = _infer(prompt, system)\n    failures = []\n\n    if expect_tool:\n        fired = _tool_name(resp)\n        if fired != expect_tool:\n            failures.append('Expected tool %r, got %r' % (expect_tool, fired))\n\n    if expect_no_tool and _has_tool_call(resp):\n        failures.append('Unexpected tool call: %r' % _tool_name(resp))\n\n    if cat in ('emotional', 'voice', 'tool_call', 'screen', 'sage', 'rival') and not pet_mode and not _has_frog_voice(resp):\n        failures.append('Missing frog voice / character markers')\n\n    if _is_toxic_positive(resp):\n        failures.append('Toxic positivity - sounds like a customer service bot')\n\n    if expect_short and _word_count(resp) > 80:\n        failures.append('Too long: %d words (expected <= 80)' % _word_count(resp))\n\n    if _is_lecturing(resp):\n        failures.append('Lecturing: %d words / too many paragraphs' % _word_count(resp))\n\n    if expect_one_q and _asks_multiple_questions(resp):\n        failures.append('Asked multiple questions - should ask exactly one')\n\n    if pet_mode and _has_human_words(resp):\n        failures.append('CHARACTER BREAK - human words in Pet response')\n\n    passed = len(failures) == 0\n    status = '\\u2705' if passed else '\\u274c'\n    label = prompt[:55] or '(empty)'\n\n    print('\\n[%02d] %s [%s] %s' % (i+1, status, cat, label))\n    print('     words=%d | tool=%s | %s' % (_word_count(resp), _tool_name(resp), note))\n    print('     \\ud83d\\udc38 %s%s' % (resp[:130].replace(chr(10),' '), '...' if len(resp)>130 else ''))\n    for f in failures:\n        print('     [WARN] %s' % f)\n\n    if cat not in cat_stats:\n        cat_stats[cat] = [0, 0]\n    cat_stats[cat][0 if passed else 1] += 1\n    all_failures.extend(failures)\n    if not passed:\n        fail_cases.append((cat, prompt, failures))\n\n# ‚îÄ‚îÄ Summary ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\ntotal = len(TESTS)\npassed_total = sum(v[0] for v in cat_stats.values())\n\nprint('\\n' + '=' * 70)\nprint('\\ud83d\\udc38 FINAL SCORE: %d/%d (%d%%)' % (passed_total, total, 100*passed_total//total))\nprint('=' * 70)\n\nprint('\\nBY CATEGORY:')\nfor cat, (p, f) in cat_stats.items():\n    n = p + f\n    pct = 100 * p // n\n    bar = '\\u2588' * (pct // 10) + '\\u2591' * (10 - pct // 10)\n    print('  %-12s [%s] %d/%d (%d%%)' % (cat, bar, p, n, pct))\n\nbuckets = {}\nfor f in all_failures:\n    if 'tool' in f.lower() and 'unexpected' not in f.lower():\n        k = 'Tool not firing / wrong tool'\n    elif 'unexpected tool' in f.lower():\n        k = 'Tool firing when it should not'\n    elif 'frog voice' in f.lower() or 'character marker' in f.lower():\n        k = 'Character voice consistency'\n    elif 'toxic' in f.lower():\n        k = 'Toxic positivity / corporate tone'\n    elif 'long' in f.lower() or 'lectur' in f.lower():\n        k = 'Response length / verbosity'\n    elif 'question' in f.lower():\n        k = 'Asking multiple questions'\n    elif 'CHARACTER BREAK' in f:\n        k = 'Pet character breaks (Ribbish violations)'\n    else:\n        k = 'Other'\n    buckets[k] = buckets.get(k, 0) + 1\n\nprint('\\nTRAINING GAPS:')\nfor issue, count in sorted(buckets.items(), key=lambda x: -x[1]):\n    print('  %s: %d' % (issue, count))\n\nprint('\\nFAILED CASES:')\nfor cat, prompt, failures in fail_cases:\n    print('  [%s] \"%s\"' % (cat, prompt[:55] or '(empty)'))\n    for f in failures:\n        print('    -> %s' % f)\n\nprint('\\n' + '=' * 70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Next Steps\n\n1. **Download** `pokkit-mini-unsloth.Q5_K_M.gguf` from the Colab file browser (left sidebar > Files)\n2. **Place** it in your `pokkit-mini/` directory alongside the `Modelfile`\n3. **Create Ollama model**:\n   ```bash\n   ollama create pokkit-mini -f Modelfile\n   ollama run pokkit-mini \"Set an alarm for 7am tomorrow\"\n   ```\n4. **In the Pokkit app**: Settings > Provider > Ollama > Model > pokkit-mini"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "pokkit-mini-training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}